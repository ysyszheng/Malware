import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torchvision import models
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
from PIL import Image
from utils.utils import fix_seed
from models.simclrmodel import SimCLRModel
from tqdm import tqdm
import numpy as np


class Trainer(object):
    def __init__(self, cfgs):
        fix_seed(cfgs['seed'])
        self.cfgs = cfgs

        base_encoder = models.resnet50(pretrained=True)
        base_encoder.fc = nn.Identity()
        self.device = f'cuda:{cfgs["cuda_idx"]}' if torch.cuda.is_available() else 'cpu'
        self.model = SimCLRModel(base_encoder, projection_dim=cfgs['projection_dim']).to(self.device)

        self.optimizer = optim.Adam(self.model.parameters(), lr=cfgs["learning_rate"], weight_decay=cfgs['weight_decay'])
        self.criterion = nn.CrossEntropyLoss()
        self.model.to(self.device)


    def training(self):
        self.model.train()
        train_loss_list = []
        
        progress_bar = tqdm(range(1, self.cfgs['num_epochs']+1))
        for epoch in progress_bar:
            running_loss = 0.0
            for images, _ in self.dataloaders:
                images = images.to(self.device)

                transform = transforms.Compose([
                    transforms.RandomHorizontalFlip(),
                    transforms.RandomResizedCrop(224, scale=(0.08, 1.0)),
                    transforms.ToTensor(),
                ])

                images_aug = torch.stack([transform(img) for img in images])

                self.optimizer.zero_grad()

                features = self.model(images_aug)

                batch_size = images.size(0)
                labels = torch.arange(batch_size, device=self.device)
                labels = torch.cat([labels, labels], dim=0)
                logits = torch.div(torch.matmul(features, features.T), 0.07)
                loss = self.criterion(logits, labels)

                loss.backward()
                self.optimizer.step()
                running_loss += loss.item() * labels.size(0)

            epoch_loss = running_loss / len(self.dataloaders)
            train_loss_list.append(epoch_loss)
            progress_bar.set_description(
                    f'Epoch {epoch}, loss: {epoch_loss:.4f}, lr: {self.optimizer.param_groups[0]["lr"]:.6f}')

            if epoch % self.cfgs['save_freq'] == 0:
                torch.save(self.model.state_dict(), f'{self.cfgs["save_model_path"]}/{self.cfgs["model"]}_{epoch}.pth')
                np.save(f'{self.cfgs["save_data_path"]}/{self.cfgs["model"]}_train_loss_{epoch}.npy', train_loss_list)
