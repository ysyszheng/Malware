import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.optim.lr_scheduler import ExponentialLR, StepLR
from torch.optim import SGD, Adam
from models.cnnmodels import VGG16, ResNet50, GoogLeNet, DenseNet121, ShuffleNet
from utils.utils import fix_seed, log
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from PIL import Image
Image.MAX_IMAGE_PIXELS = None
import pickle


class Ensembler(object):
    def __init__(self, cfgs):
        fix_seed(cfgs['seed'])
        self.cfgs = cfgs

        self.vgg16 = VGG16(num_classes=cfgs["num_classes"])
        self.resnet50 = ResNet50(num_classes=cfgs["num_classes"])
        self.googlenet = GoogLeNet(num_classes=cfgs["num_classes"])
        self.densenet121 = DenseNet121(num_classes=cfgs["num_classes"])
        self.shufflenet = ShuffleNet(num_classes=cfgs["num_classes"])

        self.device = f'cuda:{cfgs["cuda_idx"]}' if (torch.cuda.is_available() and cfgs["cuda_idx"]>=0) else 'cpu'
        self.vgg16.to(self.device)
        self.resnet50.to(self.device)
        self.googlenet.to(self.device)
        self.densenet121.to(self.device)
        self.shufflenet.to(self.device)

        self.vgg16.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/vgg16_best.pth'))
        self.resnet50.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/resnet50_best.pth'))
        self.googlenet.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/googlenet_best.pth'))
        self.densenet121.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/densenet121_best.pth'))
        self.shufflenet.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/shufflenet_best.pth'))

        self.vgg16.eval()
        self.resnet50.eval()
        self.googlenet.eval()
        self.densenet121.eval()
        self.shufflenet.eval()

        self.dataloaders = dict()
        self.dataloaders['train'] = torch.load(f'{cfgs["save_data_path"]}/train_loader.pth')
        self.dataloaders['val'] = torch.load(f'{cfgs["save_data_path"]}/val_loader.pth')
        self.dataloaders['test'] = torch.load(f'{cfgs["save_data_path"]}/test_loader.pth')

    def majority_voting(self):
        true_labels = []
        predicted_labels = []

        with torch.no_grad():
            for inputs, labels in self.dataloaders['test']:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                vgg16_outputs = F.softmax(self.vgg16(inputs), dim=1)
                resnet50_outputs = F.softmax(self.resnet50(inputs), dim=1)
                googlenet_outputs = F.softmax(self.googlenet(inputs), dim=1)
                densenet_outputs = F.softmax(self.densenet121(inputs), dim=1)
                shufflenet_outputs = F.softmax(self.shufflenet(inputs), dim=1)

                vgg16_preds = torch.argmax(vgg16_outputs, dim=1)
                resnet50_preds = torch.argmax(resnet50_outputs, dim=1)
                googlenet_preds = torch.argmax(googlenet_outputs, dim=1)
                densenet_preds = torch.argmax(densenet_outputs, dim=1)
                shufflenet_preds = torch.argmax(shufflenet_outputs, dim=1)

                all_preds = torch.stack([vgg16_preds, resnet50_preds, googlenet_preds, densenet_preds, shufflenet_preds], dim=0)
                preds, _ = torch.mode(all_preds, dim=0)
                true_labels.extend(labels.cpu().tolist())
                predicted_labels.extend(preds.cpu().tolist())

            acc = sum(true_label == predicted_label for true_label, predicted_label in zip(true_labels, predicted_labels)) / len(true_labels)
            print(f'majority voting accuracy: {acc}')
            conf_matrix = confusion_matrix(true_labels, predicted_labels)
            np.save(f'{self.cfgs["save_data_path"]}/majority_voting_conf_matrix.npy', conf_matrix)

    def distribution_summation(self):
        true_labels = []
        predicted_labels = []

        with torch.no_grad():
            for inputs, labels in self.dataloaders['test']:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                vgg16_outputs = F.softmax(self.vgg16(inputs), dim=1)
                resnet50_outputs = F.softmax(self.resnet50(inputs), dim=1)
                googlenet_outputs = F.softmax(self.googlenet(inputs), dim=1)
                densenet_outputs = F.softmax(self.densenet121(inputs), dim=1)
                shufflenet_outputs = F.softmax(self.shufflenet(inputs), dim=1)

                all_preds = vgg16_outputs + resnet50_outputs + googlenet_outputs + densenet_outputs + shufflenet_outputs
                _, preds = torch.max(all_preds, dim=1) 
                true_labels.extend(labels.cpu().tolist())
                predicted_labels.extend(preds.cpu().tolist())

            acc = sum(true_label == predicted_label for true_label, predicted_label in zip(true_labels, predicted_labels)) / len(true_labels)
            print(f'distribution summation accuracy: {acc}')
            conf_matrix = confusion_matrix(true_labels, predicted_labels)
            np.save(f'{self.cfgs["save_data_path"]}/distribution_summation_conf_matrix.npy', conf_matrix)


    def bayesian_combination(self):
        true_labels = []
        predicted_labels = []
        
        vgg16_conf_matrix = np.load(f'{self.cfgs["save_data_path"]}/vgg16_val_conf_matrix.npy')
        resnet50_conf_matrix = np.load(f'{self.cfgs["save_data_path"]}/resnet50_val_conf_matrix.npy')
        googlenet_conf_matrix = np.load(f'{self.cfgs["save_data_path"]}/googlenet_val_conf_matrix.npy')
        densenet121_conf_matrix = np.load(f'{self.cfgs["save_data_path"]}/densenet121_val_conf_matrix.npy')
        shufflenet_conf_matrix = np.load(f'{self.cfgs["save_data_path"]}/shufflenet_val_conf_matrix.npy')

        vgg16_conf_matrix = vgg16_conf_matrix / vgg16_conf_matrix.sum(axis=1, keepdims=True)
        resnet50_conf_matrix = resnet50_conf_matrix / resnet50_conf_matrix.sum(axis=1, keepdims=True)
        googlenet_conf_matrix = googlenet_conf_matrix / googlenet_conf_matrix.sum(axis=1, keepdims=True)
        densenet121_conf_matrix = densenet121_conf_matrix / densenet121_conf_matrix.sum(axis=1, keepdims=True)
        shufflenet_conf_matrix = shufflenet_conf_matrix / shufflenet_conf_matrix.sum(axis=1, keepdims=True)

        vgg16_probs = torch.Tensor([vgg16_conf_matrix[i,i] for i in range(len(vgg16_conf_matrix))]).to(self.device)
        resnet50_probs = torch.Tensor([resnet50_conf_matrix[i,i] for i in range(len(resnet50_conf_matrix))]).to(self.device)
        googlenet_probs = torch.Tensor([googlenet_conf_matrix[i, i] for i in range(len(googlenet_conf_matrix))]).to(self.device)
        densenet121_probs = torch.Tensor([densenet121_conf_matrix[i, i] for i in range(len(densenet121_conf_matrix))]).to(self.device)
        shufflenet_probs = torch.Tensor([shufflenet_conf_matrix[i, i] for i in range(len(shufflenet_conf_matrix))]).to(self.device)

        with torch.no_grad():
            for inputs, labels in self.dataloaders['test']:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                vgg16_outputs = F.softmax(self.vgg16(inputs), dim=1)
                resnet50_outputs = F.softmax(self.resnet50(inputs), dim=1)
                googlenet_outputs = F.softmax(self.googlenet(inputs), dim=1)
                densenet_outputs = F.softmax(self.densenet121(inputs), dim=1)
                shufflenet_outputs = F.softmax(self.shufflenet(inputs), dim=1)

                outputs = vgg16_outputs * vgg16_probs + resnet50_outputs * resnet50_probs + \
                    googlenet_outputs * googlenet_probs + densenet_outputs * densenet121_probs + \
                        shufflenet_outputs * shufflenet_probs
                _, preds = torch.max(outputs, dim=1) 

                true_labels.extend(labels.cpu().tolist())
                predicted_labels.extend(preds.cpu().tolist())

            acc = sum(true_label == predicted_label for true_label, predicted_label in zip(true_labels, predicted_labels)) / len(true_labels)
            print(f'bayesian combination accuracy: {acc}')
            conf_matrix = confusion_matrix(true_labels, predicted_labels)
            np.save(f'{self.cfgs["save_data_path"]}/bayesian_combination_conf_matrix.npy', conf_matrix)

    def stacking(self):
        X_train = []
        y_train = []

        with torch.no_grad():
            for inputs, labels in self.dataloaders['val']:
                inputs = inputs.to(self.device)

                vgg16_outputs = F.softmax(self.vgg16(inputs), dim=1).detach().cpu().numpy()
                resnet50_outputs = F.softmax(self.resnet50(inputs), dim=1).detach().cpu().numpy()
                googlenet_outputs = F.softmax(self.googlenet(inputs), dim=1).detach().cpu().numpy()
                densenet_outputs = F.softmax(self.densenet121(inputs), dim=1).detach().cpu().numpy()
                shufflenet_outputs = F.softmax(self.shufflenet(inputs), dim=1).detach().cpu().numpy()

                new_features = np.hstack((vgg16_outputs, resnet50_outputs, googlenet_outputs, densenet_outputs, shufflenet_outputs))
                X_train.extend(new_features)
                y_train.extend(labels.tolist())

        X_train = np.array(X_train)
        y_train = np.array(y_train)

        meta_model = LogisticRegression()
        meta_model.fit(X_train, y_train)
        with open(f'{self.cfgs["save_model_path"]}/stacking_meta_model.pkl', 'wb') as f:
            pickle.dump(meta_model, f)

        X_test = []
        y_test = []
        with torch.no_grad():
            for inputs, labels in self.dataloaders['test']:
                inputs = inputs.to(self.device)

                vgg16_outputs = F.softmax(self.vgg16(inputs), dim=1).detach().cpu().numpy()
                resnet50_outputs = F.softmax(self.resnet50(inputs), dim=1).detach().cpu().numpy()
                googlenet_outputs = F.softmax(self.googlenet(inputs), dim=1).detach().cpu().numpy()
                densenet_outputs = F.softmax(self.densenet121(inputs), dim=1).detach().cpu().numpy()
                shufflenet_outputs = F.softmax(self.shufflenet(inputs), dim=1).detach().cpu().numpy()

                new_features = np.hstack((vgg16_outputs, resnet50_outputs, googlenet_outputs, densenet_outputs, shufflenet_outputs))
                X_test.extend(new_features)
                y_test.extend(labels)

        X_test = np.array(X_test)

        predicted_labels = meta_model.predict(X_test)
        acc = sum(true_label == predicted_label for true_label, predicted_label in zip(y_test, predicted_labels)) / len(y_test)
        print(f'stacking accuracy: {acc}')
        conf_matrix = confusion_matrix(y_test, predicted_labels)
        np.save(f'{self.cfgs["save_data_path"]}/stacking_conf_matrix.npy', conf_matrix)

    def mlp(self):
        class MLP(nn.Module):
            def __init__(self, num_classes):
                super(MLP, self).__init__()
                self.mlp_ = nn.Sequential(
                    nn.Linear(num_classes * 5, 32),
                    nn.Linear(32, num_classes),
                )
                self.initialize_weights()
            
            def forward(self, x):
                return self.mlp_(x)

            def initialize_weights(self):
                for m in self.mlp_:
                    if isinstance(m, nn.Linear):
                        nn.init.xavier_uniform_(m.weight)
                        nn.init.zeros_(m.bias)

        mlp = MLP(self.cfgs['num_classes'])
        mlp.to(self.device)
        optimizer = Adam(mlp.parameters(), lr=1e-5)
        criterion = nn.CrossEntropyLoss()
        # scheduler = StepLR(optimizer, step_size=5, gamma=0.9)

        for epoch in range(1,51):
            mlp.train()
            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in self.dataloaders['train']:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                vgg16_outputs = self.vgg16(inputs).detach()
                resnet50_outputs = self.resnet50(inputs).detach()
                googlenet_outputs = self.googlenet(inputs).detach()
                densenet_outputs = self.densenet121(inputs).detach()
                shufflenet_outputs = self.shufflenet(inputs).detach()
                inputs = torch.hstack((vgg16_outputs, resnet50_outputs, googlenet_outputs, densenet_outputs, shufflenet_outputs))

                optimizer.zero_grad()

                with torch.set_grad_enabled(True):
                    outputs = mlp(inputs)
                    loss = criterion(outputs, labels)

                    _, preds = torch.max(outputs, dim=1)

                    loss.backward()
                    optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data).cpu().numpy()
            
            # scheduler.step()
            epoch_loss = running_loss / len(self.dataloaders['train'].dataset)
            epoch_acc = running_corrects / len(self.dataloaders['train'].dataset)
            print(f'epoch: {epoch}, loss: {epoch_loss}, acc: {epoch_acc}, lr: {optimizer.param_groups[0]["lr"]}')

        mlp.eval()
        true_labels = []
        predicted_labels = []

        with torch.no_grad():
            for inputs, labels in self.dataloaders['test']:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                vgg16_outputs = self.vgg16(inputs).detach()
                resnet50_outputs = self.resnet50(inputs).detach()
                googlenet_outputs = self.googlenet(inputs).detach()
                densenet_outputs = self.densenet121(inputs).detach()
                shufflenet_outputs = self.shufflenet(inputs).detach()
                inputs = torch.hstack((vgg16_outputs, resnet50_outputs, googlenet_outputs, densenet_outputs, shufflenet_outputs))

                outputs = mlp(inputs)
                _, preds = torch.max(outputs, 1)

                true_labels.extend(labels.cpu().numpy())
                predicted_labels.extend(preds.cpu().numpy())

        conf_matrix = confusion_matrix(true_labels, predicted_labels)
        accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)
        print(f"test accuracy: {accuracy}")