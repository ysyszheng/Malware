import torch
import torch.nn.functional as F
import numpy as np
from config.cfgs import cfgs
from models.cnnmodels import VGG16, ResNet50, GoogLeNet, DenseNet121, ShuffleNet
from utils.utils import fix_seed, log
from sklearn.metrics import confusion_matrix
from torch.utils.data import DataLoader, random_split
from sklearn.linear_model import LogisticRegression


class Ensembler(object):
    def __init__(self, cfgs):
        fix_seed(cfgs['seed'])
        self.cfgs = cfgs

        self.vgg16 = VGG16(num_classes=cfgs["num_classes"])
        self.resnet50 = ResNet50(num_classes=cfgs["num_classes"])
        self.googlenet = GoogLeNet(num_classes=cfgs["num_classes"])
        self.densenet121 = DenseNet121(num_classes=cfgs["num_classes"])
        self.shufflenet = ShuffleNet(num_classes=cfgs["num_classes"])

        self.device = f'cuda:{cfgs["cuda_idx"]}' if torch.cuda.is_available() else 'cpu'
        self.vgg16.to(self.device)
        self.resnet50.to(self.device)
        self.googlenet.to(self.device)
        self.densenet121.to(self.device)
        self.shufflenet.to(self.device)

        # TODO: which models to use
        self.vgg16.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/vgg16_20.pth'))
        self.resnet50.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/resnet50_20.pth'))
        self.googlenet.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/googlenet_20.pth'))
        self.densenet121.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/densenet121_20.pth'))
        self.shufflenet.load_state_dict(torch.load(f'{cfgs["save_model_path"]}/shufflenet_20.pth'))

        self.vgg16.eval()
        self.resnet50.eval()
        self.googlenet.eval()
        self.densenet121.eval()
        self.shufflenet.eval()

        self.dataloaders = dict()
        self.dataloaders['train'] = torch.load(f'{cfgs["save_data_path"]}/train_loader.pth')
        self.dataloaders['val'] = torch.load(f'{cfgs["save_data_path"]}/val_loader.pth')

    def bayesian_combination(self):
        true_labels = []
        predicted_labels = []
        
        vgg16_conf_matrix = np.load(f'{cfgs["save_data_path"]}/vgg16_train_conf_matrix.npy')
        resnet50_conf_matrix = np.load(f'{cfgs["save_data_path"]}/resnet50_train_conf_matrix.npy')
        googlenet_conf_matrix = np.load(f'{cfgs["save_data_path"]}/googlenet_train_conf_matrix.npy')
        densenet121_conf_matrix = np.load(f'{cfgs["save_data_path"]}/densenet121_train_conf_matrix.npy')
        shufflenet_conf_matrix = np.load(f'{cfgs["save_data_path"]}/shufflenet_train_conf_matrix.npy')

        vgg16_conf_matrix = vgg16_conf_matrix / vgg16_conf_matrix.sum(axis=1, keepdims=True)
        resnet50_conf_matrix = resnet50_conf_matrix / resnet50_conf_matrix.sum(axis=1, keepdims=True)
        googlenet_conf_matrix = googlenet_conf_matrix / googlenet_conf_matrix.sum(axis=1, keepdims=True)
        densenet121_conf_matrix = densenet121_conf_matrix / densenet121_conf_matrix.sum(axis=1, keepdims=True)
        shufflenet_conf_matrix = shufflenet_conf_matrix / shufflenet_conf_matrix.sum(axis=1, keepdims=True)

        vgg16_probs = torch.Tensor([vgg16_conf_matrix[i,i] for i in range(len(vgg16_conf_matrix))]).to(self.device)
        resnet50_probs = torch.Tensor([resnet50_conf_matrix[i,i] for i in range(len(resnet50_conf_matrix))]).to(self.device)
        googlenet_probs = torch.Tensor([googlenet_conf_matrix[i, i] for i in range(len(googlenet_conf_matrix))]).to(self.device)
        densenet121_probs = torch.Tensor([densenet121_conf_matrix[i, i] for i in range(len(densenet121_conf_matrix))]).to(self.device)
        shufflenet_probs = torch.Tensor([shufflenet_conf_matrix[i, i] for i in range(len(shufflenet_conf_matrix))]).to(self.device)

        with torch.no_grad():
            for inputs, labels in self.dataloaders['val']:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                vgg16_outputs = F.softmax(self.vgg16(inputs), dim=1)
                resnet50_outputs = F.softmax(self.resnet50(inputs), dim=1)
                googlenet_outputs = F.softmax(self.googlenet(inputs), dim=1)
                densenet_outputs = F.softmax(self.densenet121(inputs), dim=1)
                shufflenet_outputs = F.softmax(self.shufflenet(inputs), dim=1)

                outputs = vgg16_outputs * vgg16_probs + resnet50_outputs * resnet50_probs + \
                    googlenet_outputs * googlenet_probs + densenet_outputs * densenet121_probs + \
                        shufflenet_outputs * shufflenet_probs
                _, preds = torch.max(outputs, dim=1) 

                true_labels.extend(labels.cpu().tolist())
                predicted_labels.extend(preds.cpu().tolist())

            acc = sum(true_label == predicted_label for true_label, predicted_label in zip(true_labels, predicted_labels)) / len(true_labels)
            print(f'bayesian combination accuracy: {acc}')
            conf_matrix = confusion_matrix(true_labels, predicted_labels)
            np.save(f'{cfgs["save_data_path"]}/bayesian_combination_conf_matrix.npy', conf_matrix)

    def stacking(self):
        total_size = len(self.dataloaders['val'].dataset)
        train_size = int(total_size*2/3)
        val_size = total_size - train_size

        train_dataset, val_dataset = random_split(self.dataloaders['val'].dataset, [train_size, val_size])

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)

        X_train = []
        y_train = []

        for inputs, labels in train_loader:
            inputs = inputs.to(self.device)

            vgg16_outputs = F.softmax(self.vgg16(inputs), dim=1).detach().cpu().numpy()
            resnet50_outputs = F.softmax(self.resnet50(inputs), dim=1).detach().cpu().numpy()
            googlenet_outputs = F.softmax(self.googlenet(inputs), dim=1).detach().cpu().numpy()
            densenet_outputs = F.softmax(self.densenet121(inputs), dim=1).detach().cpu().numpy()
            shufflenet_outputs = F.softmax(self.shufflenet(inputs), dim=1).detach().cpu().numpy()

            new_features = np.hstack((vgg16_outputs, resnet50_outputs, googlenet_outputs, densenet_outputs, shufflenet_outputs))
            X_train.extend(new_features)
            y_train.extend(labels.tolist())

        X_train = np.array(X_train)
        y_train = np.array(y_train)

        meta_model = LogisticRegression()
        meta_model.fit(X_train, y_train)

        X_test = []
        y_test = []
        for inputs, labels in val_loader:
            inputs = inputs.to(self.device)

            vgg16_outputs = F.softmax(self.vgg16(inputs), dim=1).detach().cpu().numpy()
            resnet50_outputs = F.softmax(self.resnet50(inputs), dim=1).detach().cpu().numpy()
            googlenet_outputs = F.softmax(self.googlenet(inputs), dim=1).detach().cpu().numpy()
            densenet_outputs = F.softmax(self.densenet121(inputs), dim=1).detach().cpu().numpy()
            shufflenet_outputs = F.softmax(self.shufflenet(inputs), dim=1).detach().cpu().numpy()

            new_features = np.hstack((vgg16_outputs, resnet50_outputs, googlenet_outputs, densenet_outputs, shufflenet_outputs))
            X_test.extend(new_features)
            y_test.extend(labels)

        X_test = np.array(X_test)

        predicted_labels = meta_model.predict(X_test)
        acc = sum(true_label == predicted_label for true_label, predicted_label in zip(y_test, predicted_labels)) / len(y_test)
        print(f'stacking accuracy: {acc}')
        conf_matrix = confusion_matrix(y_test, predicted_labels)
        np.save(f'{cfgs["save_data_path"]}/stacking_conf_matrix.npy', conf_matrix)
