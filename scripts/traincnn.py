import torch
from torch.nn import CrossEntropyLoss
from torch.optim.lr_scheduler import ExponentialLR, StepLR
from torch.optim import SGD, Adam
import os
from utils.early_stopping import EarlyStopping
import numpy as np
from tqdm import tqdm
from models.cnnmodels import VGG16, ResNet50, GoogLeNet, DenseNet121, ShuffleNet
from utils.utils import fix_seed
from sklearn.metrics import confusion_matrix
from PIL import Image
Image.MAX_IMAGE_PIXELS = None


class Trainer(object):
    def __init__(self, cfgs):
        fix_seed(cfgs['seed'])
        self.cfgs = cfgs

        if cfgs['model'] == 'vgg16':
            self.model = VGG16(num_classes=cfgs["num_classes"])
        elif cfgs['model'] == 'resnet50':
            self.model = ResNet50(num_classes=cfgs["num_classes"])
        elif cfgs['model'] == 'googlenet':
            self.model = GoogLeNet(num_classes=cfgs["num_classes"])
        elif cfgs['model'] == 'densenet121':
            self.model = DenseNet121(num_classes=cfgs["num_classes"])
        elif cfgs['model'] == 'shufflenet':
            self.model = ShuffleNet(num_classes=cfgs["num_classes"])
        else:
            raise NotImplementedError

        self.device = f'cuda:{cfgs["cuda_idx"]}' if torch.cuda.is_available() else 'cpu'
        self.model.to(self.device)

        self.optimizer = SGD(self.model.parameters(), lr=cfgs["learning_rate"], momentum=cfgs['momentum'], weight_decay=cfgs['weight_decay'])
        # self.optimizer = Adam(self.model.parameters(), lr=cfgs["learning_rate"], weight_decay=cfgs['weight_decay'])
        self.criterion = CrossEntropyLoss()
        # self.scheduler = ExponentialLR(self.optimizer, gamma=0.95)
        self.scheduler = StepLR(self.optimizer, step_size=cfgs['step_size'], gamma=cfgs['gamma'])
        self.early_stopping = EarlyStopping(patience=cfgs['patience'], delta=cfgs['delta'])

        self.dataloaders = dict()
        self.dataloaders['train'] = torch.load(f'{cfgs["save_data_path"]}/train_loader.pth')
        self.dataloaders['val'] = torch.load(f'{cfgs["save_data_path"]}/val_loader.pth')
        self.dataloaders['test'] = torch.load(f'{cfgs["save_data_path"]}/test_loader.pth')

    def training(self):
        self.model.train()
        train_loss_list = []
        val_loss_list = []
        train_acc_list = []
        val_acc_list = []

        best_model_params_path = f'{self.cfgs["save_model_path"]}/{self.cfgs["model"]}_best.pth'
        torch.save(self.model.state_dict(), best_model_params_path)
        best_acc = 0.0

        progress_bar = tqdm(range(1, self.cfgs['num_epochs']+1))
        for epoch in progress_bar:
            for phase in ['train', 'val']:
                if phase == 'train':
                    self.model.train()
                else:
                    self.model.eval()

                running_loss = 0.0
                running_corrects = 0

                for inputs, labels in self.dataloaders[phase]:
                    inputs, labels = inputs.to(self.device), labels.to(self.device)

                    self.optimizer.zero_grad()

                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = self.model(inputs)
                        loss = self.criterion(outputs, labels)

                        _, preds = torch.max(outputs, dim=1)

                        if phase == 'train':
                            loss.backward()
                            self.optimizer.step()

                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data).cpu().numpy()

                epoch_loss = running_loss / len(self.dataloaders[phase].dataset)
                epoch_acc = running_corrects / len(self.dataloaders[phase].dataset)
                progress_bar.set_description(
                    f'Epoch {epoch}, {phase} loss: {epoch_loss:.4f}, {phase} accuracy: {epoch_acc:.4f}, lr: {self.optimizer.param_groups[0]["lr"]:.7f}')

                if phase == 'train':
                    train_loss_list.append(epoch_loss)
                    train_acc_list.append(epoch_acc)
                else: # phase == 'val'
                    val_loss_list.append(epoch_loss)
                    val_acc_list.append(epoch_acc)
                    self.early_stopping(epoch_loss)

                    if epoch_acc > best_acc:
                        best_acc = epoch_acc
                        torch.save(self.model.state_dict(), best_model_params_path)
            
            self.scheduler.step()

            if epoch % self.cfgs['save_freq'] == 0:
                torch.save(self.model.state_dict(), f'{self.cfgs["save_model_path"]}/{self.cfgs["model"]}_{epoch}.pth')
                np.save(f'{self.cfgs["save_data_path"]}/{self.cfgs["model"]}_train_loss_{epoch}.npy', train_loss_list)
                np.save(f'{self.cfgs["save_data_path"]}/{self.cfgs["model"]}_val_loss_{epoch}.npy', val_loss_list)
                np.save(f'{self.cfgs["save_data_path"]}/{self.cfgs["model"]}_train_acc_{epoch}.npy', train_acc_list)
                np.save(f'{self.cfgs["save_data_path"]}/{self.cfgs["model"]}_val_acc_{epoch}.npy', val_acc_list)
            
            if self.early_stopping.early_stop:
                progress_bar.set_description(f"Epoch {epoch}, early stopping")
                break


    def testing(self):
        self.model.load_state_dict(torch.load(f'{self.cfgs["save_model_path"]}/{self.cfgs["model"]}_best.pth'))
        for phase in ['train', 'val', 'test']:
            self.model.eval()
            true_labels = []
            predicted_labels = []

            with torch.no_grad():
                for inputs, labels in self.dataloaders[phase]:
                    inputs, labels = inputs.to(self.device), labels.to(self.device)

                    outputs = self.model(inputs)
                    _, preds = torch.max(outputs, 1)

                    true_labels.extend(labels.cpu().numpy())
                    predicted_labels.extend(preds.cpu().numpy())

            conf_matrix = confusion_matrix(true_labels, predicted_labels)
            accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)
            print(f"{phase} accuracy: {accuracy}")
            np.save(f'{self.cfgs["save_data_path"]}/{self.cfgs["model"]}_{phase}_conf_matrix.npy', conf_matrix)
